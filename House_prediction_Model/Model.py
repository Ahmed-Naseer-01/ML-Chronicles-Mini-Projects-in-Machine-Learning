# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xp37laLmwUJXc2--l9Knf_z-ZLBliZTf
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np

# Matplotlib for visualization
from matplotlib import pyplot as plt
# display plots in the notebook
# %matplotlib inline 

# Seaborn for easier visualization
import seaborn as sns

# Scikit-Learn for Modeling
import sklearn

# Import Elastic Net, Ridge Regression, and Lasso Regression
from sklearn.linear_model import ElasticNet, Ridge, Lasso

# Import Random Forest and Gradient Boosted Trees
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

from sklearn.svm import SVC

# Helper for cross-validation
from sklearn.model_selection import GridSearchCV

# Function for splitting training and test set
from sklearn.model_selection import train_test_split

"""#Data pre-processing"""

import pandas as pd
df=pd.read_csv('/content/kc_house_data.csv')

df.head()

"""formatting of date and add it into new columns Date and year"""

df['Date']=pd.to_datetime(df['date'],format="%Y/%m/%d")
df['year'] = df['Date'].dt.year
df['property_age']=df['year']-df['yr_built']

df['zipcode_f'] = df['zipcode'].astype('category')

len(df['zipcode'].unique())

"""**one-hot encoding or dummy variable encoding**"""

dff =df.drop(['id','date','Date','yr_built','yr_renovated','year','zipcode'],axis=1)

row1,col1=dff.shape

dff = pd.get_dummies(dff, columns=['zipcode_f'])

dff.head()

dff.shape

"""#data Split"""

from sklearn.model_selection import train_test_split
target=dff['price']
features=dff.drop(['price'],axis=1)
X_train,X_test,y_train,y_test=train_test_split(features,target,test_size=0.2,random_state=123)

X_test.to_csv('test.csv',index=False)
y_test.to_csv('test_value',index=False)

print( len(X_train), len(X_test), len(y_train), len(y_test) )

"""#model Training"""

from sklearn.pipeline import make_pipeline

pipelines = {
    'lasso' : make_pipeline(Lasso(random_state=123)),
    'ridge' : make_pipeline(Ridge(random_state=123)),
    'enet' : make_pipeline(ElasticNet(random_state=123)),
    'rf' : make_pipeline(RandomForestRegressor(random_state=123)),
    'gb': make_pipeline(GradientBoostingRegressor(random_state=123))    
}

for key, value in pipelines.items():
    print(key, type(value))
print('length',len(pipelines))

# Lasso hyperparameters
lasso_hyperparameters = { 
    'lasso__alpha' : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10] 
}

# Ridge hyperparameters
ridge_hyperparameters = { 
    'ridge__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]  
}
# Elastic Net hyperparameters
enet_hyperparameters = { 
    'elasticnet__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],                        
    'elasticnet__l1_ratio' : [0.1, 0.3, 0.5, 0.7, 0.9]  
}

# Random forest hyperparameters
rf_hyperparameters = {'randomforestregressor__n_estimators': [100, 200],
                     'randomforestregressor__max_features': ['auto', 'sqrt', 0.33]}

# Boosted tree hyperparameters
gb_hyperparameters={'gradientboostingregressor__n_estimators': [100, 200],
                   'gradientboostingregressor__learning_rate': [0.05, 0.1, 0.2],
                   'gradientboostingregressor__max_depth': [1, 3, 5]}

# Create hyperparameters dictionary
hyperparameters={'rf' : rf_hyperparameters,'lasso' : lasso_hyperparameters,'gb':gb_hyperparameters,
                 'ridge':ridge_hyperparameters,'enet':enet_hyperparameters}

for key in ['enet', 'gb', 'ridge', 'rf', 'lasso']:
    if key in hyperparameters:
        if type(hyperparameters[key]) is dict:
            print( key, 'was found in hyperparameters, and it is a grid.' )
        else:
            print( key, 'was found in hyperparameters, but it is not a grid.' )
    else:
        print( key, 'was not found in hyperparameters')

"""#fit Model"""

import torch

# Import necessary libraries
import torch
from sklearn.utils import parallel_backend

# Create empty dictionary called fitted_models
fitted_models = {}

# Loop through model pipelines, tuning each one and saving it to fitted_models
for name, pipeline in pipelines.items():
    # Create cross-validation object from pipeline and hyperparameters
    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)
    
    # Fit model on X_train, y_train with CPU or GPU acceleration
    with parallel_backend('multiprocessing'):
        model.fit(X_train, y_train)
    
    # Store model in fitted_models[name] 
    fitted_models[name] = model
    
    # Print '{name} has been fitted'
    print(name, 'has been fitted.')

# Check that we have 5 cross-validation objects
for key, value in fitted_models.items():
    print( key, type(value) )

from sklearn.exceptions import NotFittedError

for name, model in fitted_models.items():
    try:
        pred = model.predict(X_test)
        print(name, 'has been fitted.')
    except NotFittedError as e:
        print(repr(e))

for name, model in fitted_models.items():
    print( name, model.best_score_ )

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
# Display fitted random forest object
fitted_models['rf']

"""#Evaluation """

# Predict test set using fitted random forest
pred = fitted_models['rf'].predict(X_test)

# Calculate and print R^2 and MAE
print( 'R^2:', r2_score(y_test, pred ))
print( 'MAE:', mean_absolute_error(y_test, pred))

for name, model in fitted_models.items():
    pred = model.predict(X_test)
    print( name )
    print( '--------' )
    print( 'R^2:', r2_score(y_test, pred ))
    print( 'MAE:', mean_absolute_error(y_test, pred))
    print()

gb_pred = fitted_models['rf'].predict(X_test)
plt.scatter(gb_pred, y_test)
plt.xlabel('predicted')
plt.ylabel('actual')
plt.show()

gb_pred = fitted_models['lasso'].predict(X_test)
plt.scatter(gb_pred, y_test)
plt.xlabel('predicted')
plt.ylabel('actual')
plt.show()

"""#Saving Model"""

# Pickle for saving model files
import pickle

# Save winning model as final_model.pkl
with open('model_luther.pkl', 'wb') as f:
    pickle.dump(fitted_models['enet'].best_estimator_, f)

